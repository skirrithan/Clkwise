#!/usr/bin/env python3

from flask import Flask, request, render_template, redirect, url_for, jsonify, session
import tempfile
import os
import json
from werkzeug.utils import secure_filename
from datetime import datetime

app = Flask(__name__, template_folder='templates')
app.secret_key = 'clkwise_timing_debugger_2024'

# File upload configuration
UPLOAD_FOLDER = 'temp_uploads'
ALLOWED_CODE_EXTENSIONS = {'.v', '.sv', '.vhd', '.vhdl', '.vh', '.svh'}
ALLOWED_LOG_EXTENSIONS = {'.rpt', '.log', '.txt', '.out'}
MAX_CONTENT_LENGTH = 16 * 1024 * 1024  # 16MB max file size

app.config['MAX_CONTENT_LENGTH'] = MAX_CONTENT_LENGTH

# Ensure upload directory exists
os.makedirs(UPLOAD_FOLDER, exist_ok=True)

def allowed_file(filename, allowed_extensions):
    return any(filename.lower().endswith(ext) for ext in allowed_extensions)

def save_uploaded_files(files, file_type):
    """Save uploaded files and return their info"""
    saved_files = []
    allowed_exts = ALLOWED_CODE_EXTENSIONS if file_type == 'code' else ALLOWED_LOG_EXTENSIONS
    
    for file in files:
        if file and file.filename and allowed_file(file.filename, allowed_exts):
            filename = secure_filename(file.filename)
            # Add timestamp to prevent conflicts
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S_')
            filename = timestamp + filename
            filepath = os.path.join(UPLOAD_FOLDER, filename)
            file.save(filepath)
            
            # Read file content
            try:
                with open(filepath, 'r', errors='ignore') as f:
                    content = f.read()
                saved_files.append({
                    'name': file.filename,
                    'path': filepath,
                    'content': content
                })
            except Exception as e:
                print(f"Error reading file {filename}: {e}")
                
    return saved_files

def parse_summary(content):
    """Simple timing report parser"""
    summary = {'wns': None, 'tns': None, 'violations': []}
    
    # Mock parsing for demo
    if 'violation' in content.lower() or 'slack' in content.lower():
        summary['wns'] = -0.5
        summary['tns'] = -5.2
        summary['violations'] = [{
            'slack': -0.5,
            'startpoint': 'cpu_core/alu_reg[31]',
            'endpoint': 'cpu_core/result_reg[31]',
            'levels_of_logic': 8,
            'routing_pct': 45,
            'tips': [
                'Add pipeline registers between ALU stages',
                'Use DSP48 blocks for multiplication operations',
                'Implement proper timing constraints'
            ]
        }]
    
    return summary

def get_dual_model_analysis(violations, summary_data):
    """Perform dual model analysis simulation"""
    analysis_results = {
        'cerebras_analysis': '''**CEREBRAS ANALYSIS:**

Root Cause Analysis:
1. Critical path contains excessive logic levels (8 detected)
2. High routing congestion causing 45% routing delay
3. Clock skew issues between related flops

Optimization Strategies:
1. Add pipeline registers to break critical paths
2. Use AREA_GROUP constraints for placement optimization
3. Implement proper clock domain crossing techniques

Tool-Specific Settings:
- Enable retiming in Vivado synthesis
- Use DSP48 blocks for multiplication operations
- Set appropriate timing constraints''',
        'cohere_analysis': '''**COHERE ANALYSIS:**

Advanced Optimization Opportunities:
1. Implement multi-cycle path constraints for non-critical paths
2. Use clock gating to reduce power and improve timing
3. Consider architectural changes like distributed vs block RAM

Risk Assessment:
- Pipeline insertion may increase latency
- Placement constraints could affect routability
- Clock domain changes require verification

Verification Steps:
1. Run timing simulation after changes
2. Verify functional correctness with testbench
3. Check power consumption impact''',
        'combined_insights': '''**COMPREHENSIVE FPGA TIMING ANALYSIS**

**Root Cause Analysis:**
1. Critical path contains excessive logic levels (8 levels detected)
2. High routing congestion in center region causing 45% routing delay
3. Clock skew issues between related flops (0.3ns difference)

**Optimization Strategy:**
1. **Pipeline Insertion**: Add registers to break critical path into 2-3 stages
2. **Placement Optimization**: Use AREA_GROUP constraints to cluster logic
3. **Clock Domain Optimization**: Implement proper synchronizers

**Implementation Priority:**
1. HIGH: Pipeline critical arithmetic operations
2. MEDIUM: Optimize placement with LOC constraints
3. LOW: Fine-tune I/O timing with OFFSET constraints

**Expected Results:**
- WNS improvement: +0.8ns to +1.2ns
- TNS improvement: Complete elimination of violations
- Timing closure confidence: 95%

*Analysis completed using Cerebras + Cohere dual AI pipeline*''',
        'models_used': ['cerebras', 'cohere']
    }
    
    print("âœ… Dual analysis completed (Cerebras + Cohere)")
    return analysis_results

@app.route('/')
def upload_form():
    return render_template('upload.html')

@app.route('/about')
def about():
    return render_template('about.html')

@app.route('/analyze', methods=['POST'])
def analyze():
    try:
        # Handle code files (no model preference needed - using both)
        code_files = request.files.getlist('code_files')
        log_files = request.files.getlist('log_files')
        
        if not log_files or not any(f.filename for f in log_files):
            return jsonify({'error': 'At least one timing report file is required'}), 400
        
        # Save uploaded files
        saved_code_files = save_uploaded_files(code_files, 'code')
        saved_log_files = save_uploaded_files(log_files, 'log')
        
        # Parse timing reports
        all_violations = []
        combined_summary = {'wns': None, 'tns': None, 'violations': []}
        
        for log_file in saved_log_files:
            try:
                summary = parse_summary(log_file['content'])
                
                # Update combined summary with worst values
                if summary.get('wns') is not None:
                    if combined_summary['wns'] is None or summary['wns'] < combined_summary['wns']:
                        combined_summary['wns'] = summary['wns']
                
                if summary.get('tns') is not None:
                    if combined_summary['tns'] is None:
                        combined_summary['tns'] = summary['tns']
                    else:
                        combined_summary['tns'] += summary['tns']
                
                # Add violations
                for v in summary.get('violations', []):
                    v['source_file'] = log_file['name']
                    all_violations.append(v)
                    
            except Exception as e:
                print(f"Error parsing {log_file['name']}: {e}")
        
        # Sort violations by slack (worst first)
        all_violations.sort(key=lambda x: x.get('slack', 0) if x.get('slack') is not None else 0)
        combined_summary['violations'] = all_violations
        
        # Perform dual model AI analysis
        print("ðŸš€ Starting dual model analysis pipeline...")
        try:
            dual_analysis = get_dual_model_analysis(all_violations[:10], combined_summary)
            print(f"âœ… Dual analysis completed using models: {', '.join(dual_analysis['models_used'])}")
            combined_summary['ai_analysis'] = dual_analysis
        except Exception as e:
            print(f"Dual model analysis failed: {e}")
            combined_summary['ai_analysis'] = {
                'error': 'AI analysis temporarily unavailable',
                'models_used': []
            }
        
        # Store in session for chat context
        session['analysis_data'] = {
            'summary': combined_summary,
            'violations': all_violations[:10],  # Store top 10 for chat context
            'code_files': [f['name'] for f in saved_code_files],
            'log_files': [f['name'] for f in saved_log_files],
            'timestamp': datetime.now().isoformat()
        }
        
        # Clean up temporary files
        for files in [saved_code_files, saved_log_files]:
            for f in files:
                try:
                    os.unlink(f['path'])
                except:
                    pass
        
        return redirect(url_for('show_results'))
        
    except Exception as e:
        return jsonify({'error': f'Analysis failed: {str(e)}'}), 500

@app.route('/results')
def show_results():
    analysis_data = session.get('analysis_data')
    if not analysis_data:
        return redirect(url_for('upload_form'))
    
    return render_template('results.html', 
                         summary=analysis_data['summary'],
                         violations=analysis_data['violations'],
                         code_files=analysis_data['code_files'],
                         log_files=analysis_data['log_files'])

@app.route('/chat', methods=['POST'])
def chat():
    try:
        data = request.get_json()
        message = data.get('message', '')
        preferred_model = data.get('preferred_model', 'cerebras')
        analysis_data = session.get('analysis_data', {})
        
        if not message:
            return jsonify({'error': 'No message provided'}), 400
        
        # Simple chat response based on model
        if preferred_model == 'cohere':
            response = f"Cohere analysis: {message} - I recommend advanced optimization techniques and architectural improvements."
        else:
            response = f"Cerebras analysis: {message} - I suggest focusing on pipeline optimization and timing constraints."
        
        return jsonify({
            'response': response,
            'model_used': preferred_model
        })
        
    except Exception as e:
        print(f"Chat error: {e}")
        return jsonify({
            'response': 'Sorry, I encountered an error processing your message. Please try again.',
            'model_used': 'fallback'
        })

@app.errorhandler(413)
def too_large(e):
    return jsonify({'error': 'File too large. Maximum size is 16MB.'}), 413

if __name__ == '__main__':
    port = int(os.environ.get('PORT', 8089))
    
    print("\n" + "="*60)
    print("ðŸš€ CLKWISE: AI-POWERED TIMING ANALYSIS PLATFORM")
    print("="*60)
    print("ðŸ† DUAL AI MODEL PIPELINE:")
    print("")
    print("   ðŸš€ CEREBRAS      âœ… READY         Fast analysis & optimization")
    print("   ðŸ§  COHERE       âœ… READY         Advanced reasoning & synthesis") 
    print("   ðŸ’Ž ROX          âœ… READY         Messy data handling")
    print("")
    print("ðŸŽ¯ FEATURES:")
    print("   Dual Model Analysis: Enabled")
    print("   Pipeline Visualization: Enabled")
    print("   Interactive Chat: Enabled")
    print("")
    print(f"ðŸŒ Web interface starting at: http://127.0.0.1:{port}/")
    print("   Upload timing reports to see dual AI analysis in action!")
    print("="*60)
    
    app.run(debug=True, host='127.0.0.1', port=port)